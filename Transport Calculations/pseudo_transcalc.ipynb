{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transport Calculations (attempt #1)\n",
    "*Akira Di Sandro, 6/22/20*\n",
    "<br> In this notebook, I will be making some pseudocode for transport calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "xr.set_options(display_style='html')\n",
    "import intake\n",
    "%matplotlib inline\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Open Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_url = \"https://storage.googleapis.com/cmip6/pangeo-cmip6.json\"\n",
    "col = intake.open_esm_datastore(cat_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = col.search(source_id='GFDL-CM4', experiment_id='historical', table_id='Omon', variable_id=['uo','vo','wo'], grid_label='gn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (6/30) Oday is not available for 'uo' and 'vo'\n",
    "* Also, I don't think I need 'wo' as a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I think for the line above, I will need to find and download the CM4 dataset that I've been using but with a mask variable (look for it using variable) in order to do the transport calculations.\n",
    "* (update from 6/30) for the mask, Raf said that I can just use NaN's as an indication that there is no data because of land so just using data from non-NaN suffices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_dict = dog.to_dataset_dict(zarr_kwargs={'consolidated': True})\n",
    "list(dset_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = dset_dict['CMIP.NOAA-GFDL.GFDL-CM4.historical.Omon.gn']\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The above two lines will also be dependent on the change in line 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog1 = col.search(source_id='GFDL-CM4', experiment_id='historical', table_id='Omfx', variable_id='areacello')\n",
    "dset_dict1 = dog1.to_dataset_dict(zarr_kwargs={'consolidated': True})\n",
    "list(dset_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid1 = dset_dict1['CMIP.NOAA-GFDL.GFDL-CM4.historical.r1i1p1f1.Ofx.areacello.gn']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Open Cluster "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "from dask_gateway import Gateway\n",
    "\n",
    "\n",
    "gateway = Gateway()  # connect to Gateway\n",
    "\n",
    "cluster = gateway.new_cluster()  # create cluster\n",
    "cluster.scale(10)  # scale cluster\n",
    "\n",
    "client = Client(cluster)  # connect Client to Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Actual Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Calculations to get the U/V points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* (6/30) still need to edit these lat and lon to match the observational data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat1 = -4.49531\n",
    "lon1 = -208.21236\n",
    "lat2 = -5.989064\n",
    "lon2 = -205.234668"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isec, jsec, xsec, ysec = sectionate.create_section(grid['lon'], grid['lat'], lon1, lat1, lon2, lat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uvpoints = sectionate.transports_C.MOM6_UVpoints_from_section(isec, jsec)\n",
    "udata = []\n",
    "vdata = []\n",
    "\n",
    "for point in uvpoints:\n",
    "    pttype, i, j = point\n",
    "    i = int(i)\n",
    "    j = int(j)\n",
    "    if pttype == 'U':\n",
    "        lon = grid['lon'].isel(x=i, y=j).values\n",
    "        lat = grid['lat'].isel(x=i, y=j).values    \n",
    "    udata = np.concatenate([udata, [i, j, lon, lat]], axis=1)  #this line needs to be tested out\n",
    "    elif pttype == 'V':\n",
    "        lon = grid['lon'].isel(x=i, y=j).values\n",
    "        lat = grid['lat'].isel(x=i, y=j).values\n",
    "    vdata = np.concatenate([vdata, [i, j, lon, lat]], axis=1)  #this line needs to be tested out\n",
    "#    print(f'{point[0]}, {point[1]}, {point[2]}, {lon}, {lat}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "udata should be a 4-by-uvpoints that are u, matrix-like dataset that stores the i and j values and the corresponding longitude and latitude values for the grid point for which we care about the u point.\n",
    "<br>udata = [[i values],\n",
    "<br>         [j values],\n",
    "<br>         [lon],\n",
    "<br>         [lat]      ]\n",
    "         \n",
    "vdata should be a 4-by-uvpoints that are v, matrix-like dataset that stores the i and j values and the corresponding longitude and latitude values for the grid point for which we care about the v point.\n",
    "<br>vdata = [[i values],\n",
    "<br>         [j values],\n",
    "<br>         [lon],\n",
    "<br>         [lat]      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Using U/V points to calculate transport\n",
    "The following are the steps I cam eup with to do the calculation. <br>\n",
    "* (transport for each cell) T = u * SA (of cell face) * mask\n",
    "* SA = dy (vertical distance of cell) * depth of cell     (6/30) SA can be replaced with areacello\n",
    "* Mask = fraction that tells us how much of the cell is in water (mostly 1 but for the edges, it will be some fraction<1)\n",
    "* Do this calculation for all depths of (i,j) cell and summate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked area code from Raf's template\n",
    "#    this takes care of multiplying the SA by the mask, so all we have to do is use this area \n",
    "#    data for transport calculations\n",
    "area_masked = ds['areacello'].where(~np.isnan(ds['uo'].isel(time=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []  # array that will store transport across each point in udata\n",
    "for pt in udata:\n",
    "    i, j, lon, lat = pt\n",
    "    u = grid['uo'].isel(x=i,y=j,time=0).values\n",
    "    SAm = area_masked.isel(x=i,y=j).values  # masked surface area of cell\n",
    "    pt_total = 0\n",
    "    for row in range(len(SAm)):   # row iterates through the different depths that aren't nan\n",
    "        pt_total += u[row] * SAm[row]\n",
    "    total = np.append(total, pt_total)\n",
    "utrans = np.sum(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []  # array that will store transport across each point in vdata\n",
    "for pt in vdata:\n",
    "    i, j, lon, lat = pt\n",
    "    v = grid['vo'].isel(x=i,y=j,time=0).values\n",
    "    SAm = area_masked.isel(x=i,y=j).values  # masked surface area of cell\n",
    "    pt_total = 0\n",
    "    for row in range(len(SAm)):   # row iterates through the different depths that aren't nan\n",
    "        pt_total += v[row] * SAm[row]\n",
    "    total = np.append(total, pt_total)\n",
    "vtrans = np.sum(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport = utrans + vtrans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Using the above code to make a function that calculates the transport for each time stamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_model_transport(ds1, ds2, t_step, udata, vdata):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
